{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating GPT-3.5's internal semantic network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this notebook to explore the network and the data.\n",
    "\n",
    "### To create the word_paths.json file, run dictionary_search.py.\n",
    "### To create the word_graph.html file, run semantic_network.py.\n",
    "\n",
    "### If you want to explore the full network, you can download the word_graph.html file and open it in your browser,\n",
    "### but 10K nodes is a lot to load, so it might take a while to load. Physics is off by default to ensure quick loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pronouncing\n",
    "nltk.download('cmudict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, let's load the word paths from the json file and create the graph.\n",
    "with open(\"word_paths.json\", 'r') as f:\n",
    "    word_paths = json.load(f)\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges to the graph\n",
    "for word, paths in word_paths.items():\n",
    "    for path_word in paths:\n",
    "        G.add_edge(word, path_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get some statistics about the network.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of nodes\n",
    "print(\"Number of nodes: \", G.number_of_nodes())\n",
    "\n",
    "# Number of edges\n",
    "print(\"Number of edges: \", G.number_of_edges())\n",
    "\n",
    "# Average degree\n",
    "print(\"Average degree: \", sum([G.degree(n) for n in G.nodes()])/G.number_of_nodes())\n",
    "\n",
    "# Average clustering coefficient\n",
    "print(\"Average clustering coefficient: \", nx.average_clustering(G))\n",
    "\n",
    "# Density\n",
    "print(\"Density: \", nx.density(G))\n",
    "\n",
    "# Degree distribution\n",
    "# import collections\n",
    "# degree_sequence = sorted([d for n, d in G.degree()], reverse=True)\n",
    "# degreeCount = collections.Counter(degree_sequence)\n",
    "# deg, cnt = zip(*degreeCount.items())\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# plt.bar(deg, cnt, width=0.80, color='b')\n",
    "# plt.title(\"Degree Histogram\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.xlabel(\"Degree\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does GPT view words as similar?\n",
    "\n",
    "#### Due to the nature of the LLM generation, we can mostly group the word paths into 3 distinct categories. \n",
    "<ol>\n",
    "<li>Anagrams and Partial Anagrams/Continuations</li>\n",
    "<li>Rhyming words</li>\n",
    "<li>Semantic similarity</li>\n",
    "</ol>\n",
    "\n",
    "We can investigate each of these groups in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anagrams\n",
    "\n",
    "## A perfect anagram is one where no letters are added or removed, and the letters are just rearranged.\n",
    "\n",
    "def get_anagram_type(word, anagram):\n",
    "    \"\"\"\n",
    "    This function determines the type of anagram for a pair of words.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The original word.\n",
    "        anagram (str): The anagram of the original word.\n",
    "        \n",
    "    Returns:\n",
    "        str: The type of anagram. This will be one of the following values:\n",
    "             'REVERSAL' if the anagram is the original word reversed,\n",
    "             'SINGLE_CONSECUTIVE_SWAP' if the anagram is the result of swapping two consecutive letters in the original word,\n",
    "             'SINGLE_SWAP' if the anagram is the result of swapping two non-consecutive letters in the original word,\n",
    "             'CYCLIC' if the anagram is a cyclic permutation of the original word,\n",
    "             'COMPLEX ANAGRAM' if the anagram does not fit any of the above categories.\n",
    "    \"\"\"\n",
    "    # Check for reversal\n",
    "    if word == anagram[::-1]:\n",
    "        return \"REVERSAL\"\n",
    "    \n",
    "    # Check for single consecutive swap\n",
    "    elif any(word[i:i+2][::-1] == anagram[i:i+2] for i in range(len(word)-1)):\n",
    "        return \"SINGLE_CONSECUTIVE_SWAP\"\n",
    "    \n",
    "    # Check for single non-consecutive swap\n",
    "    elif any(word[i] == anagram[j] and word[j] == anagram[i] for i in range(len(word)) for j in range(i+1, len(word))):\n",
    "        return \"SINGLE_SWAP\"\n",
    "    \n",
    "    # Check for cyclic permutation\n",
    "    elif any(word[i:] + word[:i] == anagram for i in range(len(word))):\n",
    "        return \"CYCLIC\"\n",
    "    \n",
    "    # If none of the above conditions are met, it's a complex anagram\n",
    "    else:\n",
    "        return \"COMPLEX ANAGRAM\"\n",
    "\n",
    "\n",
    "anagram_count = 0\n",
    "anagram_data = []\n",
    "\n",
    "for word, paths in word_paths.items():\n",
    "    for idx, path_word in enumerate(paths):\n",
    "        if sorted(word) == sorted(path_word):\n",
    "            anagram_type = get_anagram_type(word, path_word)\n",
    "            anagram_count += 1\n",
    "            anagram_data.append({'Original Word': word, 'Anagram': path_word, 'Position': idx+1, 'Word Length': len(word), 'Type': anagram_type})\n",
    "\n",
    "df = pd.DataFrame(anagram_data)\n",
    "\n",
    "print(\"Number of perfect anagrams: \", anagram_count)\n",
    "print(df)\n",
    "\n",
    "## Show the positions and counts of perfect anagrams as a dictionary\n",
    "anagram_positions = dict(df['Position'].value_counts())\n",
    "print(\"Positions: \", anagram_positions)\n",
    "\n",
    "## Show the dictionary of anagram types and counts\n",
    "anagram_types = dict(df['Type'].value_counts())\n",
    "print(\"Anagram Types: \", anagram_types)\n",
    "\n",
    "## Show the dictionary of word lengths and counts\n",
    "word_lengths = dict(df['Word Length'].value_counts())\n",
    "print(\"Word Lengths: \", word_lengths, \"\\n\")\n",
    "\n",
    "#### We see that there are 114 perfect anagrams out of a total of 10,000 words.\n",
    "#### The most common position for a perfect anagram is 1, which means that the anagram is the first result in the word path.\n",
    "### Unsurprisingly, the longest perfect anagram is conservation and conversation, both of which have 12 letters. \n",
    "## Even humans make mistakes with these two words!\n",
    "print(df[df['Word Length'] == df['Word Length'].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Another common occurence is when the LLM begins to generate the word, but due to the restriction, changes to a different word.\n",
    "### The following script looks at these occurences, and the number of letters that were generated before the change.\n",
    "### If we normalise this by the length of the original word, we can define a value called the \"divergence fraction\".\n",
    "\n",
    "def get_common_prefix_length(word1, word2):\n",
    "    \"\"\"\n",
    "    This function returns the length of the common prefix between two words.\n",
    "    \n",
    "    Args:\n",
    "        word1 (str): The first word.\n",
    "        word2 (str): The second word.\n",
    "        \n",
    "    Returns:\n",
    "        int: The length of the common prefix.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < len(word1) and i < len(word2) and word1[i] == word2[i]:\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "continuation_data = []\n",
    "\n",
    "for word, paths in word_paths.items():\n",
    "    for idx, path_word in enumerate(paths):\n",
    "        common_prefix_length = get_common_prefix_length(word, path_word)\n",
    "        # Ensure the path word is not a perfect match with the original word\n",
    "        if common_prefix_length > 0 and common_prefix_length < len(word):\n",
    "            continuation_data.append({\n",
    "                'Original Word': word, \n",
    "                'Path Word': path_word, \n",
    "                'Letters Before Change': common_prefix_length,\n",
    "                'Match Position': idx+1\n",
    "            })\n",
    "\n",
    "df_continuations = pd.DataFrame(continuation_data)\n",
    "\n",
    "# Add the Divergence Fraction column\n",
    "df_continuations['Divergence Fraction'] = df_continuations['Letters Before Change'] / df_continuations['Original Word'].str.len()\n",
    "\n",
    "# How many words are in this category?\n",
    "print(\"Number of words with continuations: \", len(df_continuations))\n",
    "\n",
    "## Show the positions and counts of continuations as a dictionary\n",
    "continuation_positions = dict(df_continuations['Match Position'].value_counts())\n",
    "print(\"Positions: \", continuation_positions)\n",
    "\n",
    "## Show the dictionary of letters before change and counts\n",
    "letters_before_change = dict(df_continuations['Letters Before Change'].value_counts())\n",
    "print(\"Letters Before Change: \", letters_before_change)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### How many words have just been pluralised/singularised?\n",
    "\n",
    "\n",
    "# Function to check if a word has been singularized or pluralized\n",
    "def check_plural_singular(word1, word2):\n",
    "    \"\"\"\n",
    "    This function checks whether one word is a plural/singular form of another.\n",
    "    \n",
    "    Args:\n",
    "        word1 (str): The first word.\n",
    "        word2 (str): The second word.\n",
    "        \n",
    "    Returns:\n",
    "        str: 'Pluralised', 'Singularised' or 'None'.\n",
    "    \"\"\"\n",
    "    if word1 == word2 + 's':\n",
    "        return 'Pluralised'\n",
    "    elif word2 == word1 + 's':\n",
    "        return 'Singularised'\n",
    "    else:\n",
    "        return 'None'\n",
    "\n",
    "# Add the Plural/Singular column\n",
    "df_continuations['Plural/Singular'] = df_continuations.apply(lambda row: check_plural_singular(row['Original Word'], row['Path Word']), axis=1)\n",
    "\n",
    "# Count the number of plural/singular changes\n",
    "plural_count = df_continuations[df_continuations['Plural/Singular'] == 'Pluralised'].shape[0]\n",
    "singular_count = df_continuations[df_continuations['Plural/Singular'] == 'Singularised'].shape[0]\n",
    "\n",
    "print(\"Number of pluralised changes: \", plural_count)\n",
    "print(\"Number of singularised changes: \", singular_count)\n",
    "\n",
    "\n",
    "\n",
    "### We could view words with higher divergence fractions as being more robust somehow.\n",
    "\n",
    "df_continuations[df_continuations['Plural/Singular'] == 'None'].sort_values(by='Divergence Fraction', ascending=False).head(20)\n",
    "\n",
    "\n",
    "### How many distinct original words with continuations are there, check Original Word column\n",
    "\n",
    "print(\"Number of distinct original words with continuations: \", len(df_continuations['Original Word'].unique()))\n",
    "\n",
    "### How many of these words appear only once, twice or three times?\n",
    "\n",
    "original_word_counts = dict(df_continuations['Original Word'].value_counts())\n",
    "print(\"Number of original words with continuations that appear only once: \", len([k for k, v in original_word_counts.items() if v == 1]))\n",
    "print(\"Number of original words with continuations that appear only twice: \", len([k for k, v in original_word_counts.items() if v == 2]))\n",
    "print(\"Number of original words with continuations that appear three times: \", len([k for k, v in original_word_counts.items() if v == 3]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pronouncing\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the CMU Pronouncing Dictionary\n",
    "cmu_dict = nltk.corpus.cmudict.dict()\n",
    "\n",
    "# Function to get the phonemes of a word\n",
    "def get_phonemes(word):\n",
    "    \"\"\"\n",
    "    This function returns the phonemes of a word.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word.\n",
    "        \n",
    "    Returns:\n",
    "        list: The phonemes of the word.\n",
    "    \"\"\"\n",
    "    return cmu_dict[word][0]\n",
    "\n",
    "# Function to get the rhyming words of a word\n",
    "def get_rhyming_words(word, library='cmu'):\n",
    "    \"\"\"\n",
    "    This function returns the rhyming words of a word.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word.\n",
    "        library (str): The library to use. 'cmu' for CMU Pronouncing Dictionary, 'pronouncing' for pronouncing library.\n",
    "        \n",
    "    Returns:\n",
    "        list: The rhyming words of the word.\n",
    "    \"\"\"\n",
    "    if library == 'cmu':\n",
    "        phonemes = get_phonemes(word)\n",
    "        rhyming_words = []\n",
    "        for word, phonemes_list in cmu_dict.items():\n",
    "            if phonemes_list[0][-2:] == phonemes[-2:]:\n",
    "                rhyming_words.append(word)\n",
    "    elif library == 'pronouncing':\n",
    "        rhyming_words = pronouncing.rhymes(word)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid library name: {library}\")\n",
    "    \n",
    "    return rhyming_words\n",
    "\n",
    "# Load word_paths.json, and for each word, find the rhyming words.\n",
    "rhyming_pairs = []\n",
    "unrhymable_words = []\n",
    "for word, paths in tqdm(list(word_paths.items())):\n",
    "    for idx, path_word in enumerate(paths):\n",
    "        try:\n",
    "            if path_word in get_rhyming_words(word, library='pronouncing'): # change library to 'cmu' for CMU Pronouncing Dictionary\n",
    "                rhyming_pairs.append((word, path_word, idx+1))\n",
    "        except:\n",
    "            unrhymable_words.append(word)\n",
    "            continue\n",
    "\n",
    "rhyming_df = pd.DataFrame(rhyming_pairs, columns=['Original Word', 'Rhyming Word', 'Matching Position'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhyming_df\n",
    "\n",
    "### apparently 6k are rhyming words (but if we use the pronouncing library, it's 2.8K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write a script which can perform all of these checks and analyse them to produce a list of words and all theyve been altered.\n",
    "### Words can be in multiple categories, so we can have a column for each category and a 1 if the word is in that category and a 0 if not.\n",
    "### We can then use this to train a model to predict the category of a word *maybe not necessary*\n",
    "\n",
    "## Main thing would be to see how many words are in each category and how many words are in multiple categories, plot this as some sort of graph\n",
    "## to show clusters of words in different categories.\n",
    "\n",
    "### Currently the categories are:\n",
    "\n",
    "# 1. Anagrams\n",
    "# 2. Continuations (subset -s, -ed, -ing etc)\n",
    "# 3. Inverse Continuations (i.e mall -> ball, fall, call)\n",
    "# 3. Rhyming Words\n",
    "### Rhyming words are words that have the same or similar ending sound. For example, \"cat\" and \"hat.\" This is a phonemic similarity.\n",
    "# 4. Synonyms\n",
    "### Synonyms are words that have the same or similar meanings. For example, \"happy\" and \"glad.\"\n",
    "# 5. Antonyms\n",
    "#### Antonyms are words that have opposite meanings. For example, \"hot\" and \"cold.\"\n",
    "# 6. Homophones\n",
    "#### Homophones are words that sound the same but may have different spellings and meanings. For example, \"write\" and \"right.\"\n",
    "# 7. Homographs\n",
    "#### Homographs are words that are spelled the same but have different meanings. For example, \"lead\" meaning to guide someone, and \"lead\" meaning a metal.\n",
    "### SO this one doesn't make sense as we are only looking at words that have been changed.\n",
    "# Letter match in any position??\n",
    "\n",
    "### Orthographic similarity\n",
    "#### Orthographic similarity is when words have the same or similar spellings visually. \n",
    "\n",
    "### syncactic similarity\n",
    "### i.e do nouns become verbs, verbs become nouns, what is the chain of words that are created\n",
    "# \n",
    "#  \n",
    "### check for subwords, i.e. words that are contained within other words\n",
    "\n",
    "### Check also if they are the same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
